<!-- Badges -->
![Python](https://img.shields.io/badge/python-3.10%2B-blue.svg)

<!-- If you choose Apache-2.0 instead, use the next line and remove the MIT badge above -->
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](./LICENSE)

<!-- Replace the DOI badge after you mint a DOI via Zenodo -->
[![DOI](https://img.shields.io/badge/DOI-pending-lightgrey.svg)](#)
![Reproducible OOF](https://img.shields.io/badge/OOF-Reproducible-brightgreen.svg)
![Status](https://img.shields.io/badge/status-Research%2FPrototype-purple.svg)


# TabTransformer++ â€” Residualized, Calibrated Transformer for Tabular Data (PSâ€‘S5E9)

**TabTransformer++** extends TabTransformer for tabular ML with four practical innovations that make it work in weakâ€‘signal settings:  
1) **Residual learning** against a strong ensemble baseline,  
2) **Leakâ€‘free perâ€‘fold quantile tokenization** (incl. special bins for model predictions),  
3) **Gated value towers** that fuse standardized raw values into token embeddings, and  
4) **Foldâ€‘wise isotonic calibration** in zâ€‘space.  

On Kaggle **Playground Series S5E9**, this yields a reproducible **~0.005 OOF RMSE** gain over robust GBDT ensembles.

---

## TL;DR
- **Task:** Beatsâ€‘perâ€‘Minute regression (PSâ€‘S5E9).  
- **Why it matters:** Dataset has *tiny* signal; most models collapse to â€œpredict ~120â€. TT++ reliably extracts residual structure.  
- **Result:** Consistent OOF improvements; CVâ†”LB alignment; all artifacts (OOF/TEST) include MD5 checksums.  
- **Run:** `pip install -r requirements.txt && python scripts/06_tabtransformer_pp.py`

---

## Results (OOF RMSE)

| Stage | OOF RMSE | Î” vs prior |
|---|---:|---:|
| XGB (5â€‘seed, subâ€‘bag, nested affine) | 26.4559 | â€” |
| GPU Trio (LGB/XGB/CAT, perâ€‘fold bins, perâ€‘model iso â†’ blend â†’ iso) | 26.4533 | âˆ’0.0026 |
| Quad Ridge stack (base + DT + quad terms) | 26.4531 | âˆ’0.0002 |
| Residual ElasticNet (kicker; negative result) | 26.4536 | +0.0005 |
| Microâ€‘stack (ridge on [quad, enet]) | **26.4531** | ~0 |
| **TabTransformer++ (residualized, gated, calibrated)** | **26.4480** | **âˆ’0.0050** |

> Public LB tracked CV within expected variance. All stages write OOF/TEST CSVs with MD5s.

---

## Method Highlights

- **Residualization:** Train TT++ on `residual = y âˆ’ y_base`, where `y_base` is a strong ensemble (Quad Ridge). Stabilizes learning in lowâ€‘signal regimes.  
- **Leakâ€‘free tokenization:** Quantile bin edges computed on *train folds only*; separate highâ€‘resolution bins for `base_pred` and `dt_pred`.  
- **Gated value towers:** For each token, an MLP(1â†’Dâ†’D) embeds the standardized raw value and is fused with the token embedding via a learnable sigmoid gate.  
- **Calibration:** Perâ€‘fold **isotonic regression** in zâ€‘space; applied to OOF and test predictions.

---

## Repo Layout

```
scripts/
  00_fetch_data.py
  01_xgb_oof.py
  02_gpu_trio_oof.py
  03_quad_ridge.py
  04_residual_enet.py
  05_micro_stack.py
  06_tabtransformer_pp.py
  07_calibration_utils.py
  utils.py
src/ttpp/
  model.py          # transformer + gated value towers
  tokenization.py   # perâ€‘fold binning/digitization
  dataset.py        # PyTorch Dataset/DataLoader
  train.py          # loops, EMA, cosine schedule, gradâ€‘clip
  calibrate.py      # perâ€‘fold isotonic in zâ€‘space
  metrics.py
data/
  raw/  interim/  oof/  sub/
results/
  tables/ plots/ logs/
reproducibility/
  run_end_to_end.sh
  checksums.md
  env_report.txt
```

---

## Quick Start

```bash
git clone https://github.com/LEDazzio01/Tab_Transformer_Plus_Plus.git
cd Tab_Transformer_Plus_Plus

python -m pip install -r requirements.txt

# Place PSâ€‘S5E9 train/test CSVs under data/raw/
# or run (requires Kaggle API credentials configured locally):
# python scripts/00_fetch_data.py

# Full pipeline (baseline â†’ ensembles â†’ TT++)
python scripts/01_xgb_oof.py
python scripts/02_gpu_trio_oof.py
python scripts/03_quad_ridge.py
python scripts/05_micro_stack.py
python scripts/06_tabtransformer_pp.py
```

Artifacts are written to `data/oof/` and `data/sub/` with **MD5** appended. A consolidated list is in `reproducibility/checksums.md`. Environment snapshot is in `reproducibility/env_report.txt`.

---

## Plots to Expect (in `results/plots/`)
- CV vs LB alignment across stages  
- Calibration curves (pre/post iso per fold)  
- OOF residual histograms (base vs TT++)  
- Learning curves (train size vs OOF)  
- Correlation matrix of OOF predictions (XGB/LGB/CAT/DT/TT++)  
- Ablations (âˆ’value tower / âˆ’gating / âˆ’perâ€‘fold bins / âˆ’iso)

---

## Requirements

See `requirements.txt` (pinned versions). Typical stack:

```
numpy, pandas, scikitâ€‘learn, scipy
xgboost, lightgbm, catboost
torch (CUDA if available)
matplotlib, seaborn, tqdm
```

---

## License & Citation

- **License:** Apacheâ€‘2.0  
- **Cite this work:** add a `CITATION.cff` like:

```yaml
cff-version: 1.2.0
title: "TabTransformer++: Residualized, Calibrated Transformer for Tabular Data (PSâ€‘S5E9)"
message: "If you use this software, please cite it as below."
authors:
  - family-names: Dazzio
    given-names: L. Elaine
repository-code: "https://github.com/LEDazzio01/Tab_Transformer_Plus_Plus"
version: "0.1.0"
date-released: "2025-09-09"
```

## ðŸ“š Citation

If you use **TabTransformer++** in academic work or production, please cite:

> Dazzio, L. Elaine. *TabTransformer++: Residualized, Calibrated Transformer for Tabular Data (PS-S5E9).* GitHub, 2025.  
> https://github.com/LEDazzio01/Tab_Transformer_Plus_Plus

### BibTeX
```bibtex
@software{dazzio2025_tabtransformerpp,
  author  = {Dazzio, L. Elaine},
  title   = {TabTransformer++: Residualized, Calibrated Transformer for Tabular Data (PS-S5E9)},
  year    = {2025},
  version = {0.1.0},
  url     = {https://github.com/LEDazzio01/Tab_Transformer_Plus_Plus},
  note    = {Code, OOF/TEST artifacts, and reproducibility materials}
}

